{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4c27430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def find_opinion_by_lexicon(sens, lexicon_file, rate):\n",
    "    words_md = ['could', 'may', 'would', 'must', 'might', 'shall', 'ought', 'can']\n",
    "    find_opinion = []\n",
    "    score_dic = {}\n",
    "    with open(lexicon_file) as file_obj:\n",
    "        pri_list = json.load(file_obj)\n",
    "    for i, s in enumerate(sens):\n",
    "        score = 0\n",
    "        for ss in s:\n",
    "            if ss.lower() in pri_list or ss in pri_list:\n",
    "                score += 1\n",
    "            if ss.lower() in words_md:\n",
    "                score += 2\n",
    "        score_dic[i] = score\n",
    "    \n",
    "    sorted_dic = sorted(score_dic.items(), key=lambda item:item[1], reverse=True)\n",
    "    extract_length = int(len(sens)*rate)\n",
    "    for i, sd in enumerate(sorted_dic):\n",
    "        if i < extract_length:\n",
    "            find_opinion.append(sens[sd[0]])\n",
    "    return find_opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c7adb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1237\n",
      "856\n",
      "381\n"
     ]
    }
   ],
   "source": [
    "# load train set\n",
    "opinions = []\n",
    "drivers = []\n",
    "barriers = []\n",
    "for i in range(2, 15):\n",
    "    with open(str(i)+'_batch/opinion_token.json') as file_obj:\n",
    "        opinion = json.load(file_obj)\n",
    "        opinions.extend(opinion)\n",
    "    with open(str(i)+'_batch/driver_token.json') as file_obj:\n",
    "        driver = json.load(file_obj)\n",
    "        drivers.extend(driver)\n",
    "    with open(str(i)+'_batch/barrier_token.json') as file_obj:\n",
    "        barrier = json.load(file_obj)\n",
    "        barriers.extend(barrier)\n",
    "    \n",
    "with open('1_batch/driver_token.json') as file_obj:\n",
    "    opinion = json.load(file_obj)\n",
    "    opinions.extend(opinion) \n",
    "    drivers.extend(opinion)\n",
    "    \n",
    "with open('1_batch/barrier_token.json') as file_obj:\n",
    "    opinion = json.load(file_obj)\n",
    "    opinions.extend(opinion)\n",
    "    barriers.extend(opinion)\n",
    "\n",
    "print(len(opinions))\n",
    "print(len(drivers))\n",
    "print(len(barriers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ce19dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from utils.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from utils import construct_dict\n",
    "import os\n",
    "\n",
    "if 'opinion_dic.json' in os.listdir():\n",
    "    with open('opinion_dic.json') as file_obj:\n",
    "        opinion_dic = json.load(file_obj)\n",
    "else:\n",
    "    opinion_dic = construct_dict(opinions)\n",
    "    \n",
    "if 'lexicon_dic.json' in os.listdir():\n",
    "    with open('lexicon_dic.json') as file_obj:\n",
    "        lexicon_dic = json.load(file_obj)\n",
    "else:\n",
    "    print('please run unsupervised_lexicon.ipynb first to get lexicon_dic.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ff1fb90",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'opinion_dic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-52fe3e15cb4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mlexicon_freq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mwords_md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'could'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'may'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'would'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'must'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'might'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shall'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ought'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'can'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mod\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopinion_dic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mod\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlexicon_dic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mopinion_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'opinion_dic' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "opinion_words = []\n",
    "opinion_freq = []\n",
    "lexicon_freq = []\n",
    "words_md = ['could', 'may', 'would', 'must', 'might', 'shall', 'ought', 'can']\n",
    "for od in opinion_dic.keys():\n",
    "    if od in lexicon_dic.keys():\n",
    "        opinion_words.append(od)\n",
    "        opinion_freq.append(opinion_dic[od])\n",
    "        lexicon_freq.append(lexicon_dic[od])\n",
    "print(len(lexicon_dic.keys()), 'words in lexicon_dic')\n",
    "print(len(opinion_dic.keys()), 'words in opinion_dic')\n",
    "\n",
    "opinion_rank = {key: rank for rank, key in enumerate(sorted(opinion_dic.values()), 1)}\n",
    "lexicon_rank = {key: rank for rank, key in enumerate(sorted(lexicon_dic.values()), 1)}\n",
    "# print(opinion_rank)\n",
    "# calculate rank summation\n",
    "op_sum = 0\n",
    "lex_sum = 0\n",
    "for o in opinion_dic:\n",
    "    freq_n = opinion_dic[o]\n",
    "    op_sum += opinion_rank[freq_n]\n",
    "for o in lexicon_dic:\n",
    "    freq_n = lexicon_dic[o]\n",
    "    lex_sum += lexicon_rank[freq_n]\n",
    "    \n",
    "op_score = {}\n",
    "lex_score = {}\n",
    "# record the score \n",
    "for o in opinion_dic.keys():\n",
    "    freq_n = opinion_dic[o]\n",
    "    op_score[o] = opinion_rank[freq_n]/op_sum\n",
    "for o in lexicon_dic.keys():\n",
    "    freq_n = lexicon_dic[o]\n",
    "    lex_score[o] = lexicon_rank[freq_n]/lex_sum\n",
    "\n",
    "# sort the need word\n",
    "need_score = {}\n",
    "for o in op_score.keys():\n",
    "    if o in lex_score.keys():\n",
    "        need_score[o] = op_score[o]-lex_score[o]\n",
    "sort_rank = sorted(need_score.items(), key=lambda item:item[1], reverse=True)\n",
    "rank_list_whole = []\n",
    "for r in sort_rank:\n",
    "    if r[0] != '[' and r[0] != ']' and r[0] != '’':\n",
    "        rank_list_whole.append(r[0])\n",
    "\n",
    "rank_list = rank_list_whole[:150]\n",
    "json_str = json.dumps(rank_list, indent=4)\n",
    "with open('rank_150.json', 'w', encoding='utf-8') as json_file:\n",
    "    json_file.write(json_str)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4870e85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['recycling', 'incentive', 'product', 'EoL', 'scheme', 'collection', 'management', 'policy', 'industry', 'design', 'lack', 'economic', 'responsibility', 'regulation', 'waste', 'producer', 'government', 'business', 'recyclers', 'material', 'consumer', 'environmental', 'infrastructure', 'manufacturer', 'recycle', 'awareness', 'recovery', 'disposal', 'landfill', 'stewardship', 'stakeholder', 'raw', 'create', 'reduce', 'appropriate', 'Regulations', 'develop', 'improve', 'ensure', 'encourage', 'activity', 'economically', 'challenge', 'valuable', 'opportunity', 'benefit', 'new', 'sustainable', 'essential', 'hazardous', 'Manufacturers', 'need', 'efficient', 'avoid', 'effective', 'profitable', 'cost', 'long', 'circular', 'loop', 'issue', 'party', 'still', 'burden', 'environmentally', 'target', 'EOL', 'effort', 'make', 'proper', 'developing', 'reducing', 'promote', 'necessary', 'social', 'future', 'fully', 'uncertainty', 'viability', 'financial', 'introducing', 'job', 'robust', 'rare', 'investment', 'PV', 'mandatory', 'domestic', 'guarantee', 'driver', 'barrier', 'voluntary', 'help', 'economy', 'better', 'participant', 'unfavorable', 'installers', 'ban', 'competitive']\n"
     ]
    }
   ],
   "source": [
    "# Sort(Normalization(Opinion_freq) - Normalization(lexicon_freq)) to construct dictionary\n",
    "\n",
    "opinion_freq = np.array(opinion_freq).reshape(1, -1)\n",
    "normalizer_opinion = preprocessing.Normalizer().fit_transform(opinion_freq)\n",
    "normalizer_opinion = list(normalizer_opinion[0])\n",
    "\n",
    "lexicon_freq = np.array(lexicon_freq).reshape(1, -1)\n",
    "normalizer_lexicon = preprocessing.Normalizer().fit_transform(lexicon_freq)\n",
    "normalizer_lexicon = list(normalizer_lexicon[0])\n",
    "\n",
    "more_value = {}\n",
    "for i, w in enumerate(opinion_words):\n",
    "    more_value[w] = normalizer_opinion[i]-normalizer_lexicon[i]\n",
    "pri_words = sorted(more_value.items(), key=lambda item:item[1], reverse=True)\n",
    "pri_list_whole = []\n",
    "i = 0\n",
    "for p_set in pri_words:\n",
    "    if p_set[0] not in words_md and opinion_dic[p_set[0]]>1:\n",
    "        # print(i, p_set, opinion_dic[p_set[0]])\n",
    "        pri_list_whole.append(p_set[0])\n",
    "        i += 1\n",
    "        \n",
    "# The amount can be adjusted, here choose 100  \n",
    "pri_list = pri_list_whole[:100]\n",
    "# json_str = json.dumps(pri_list, indent=4)\n",
    "# with open('pri_200.json', 'w', encoding='utf-8') as json_file:\n",
    "    # json_file.write(json_str)\n",
    "print(pri_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60594f6a",
   "metadata": {},
   "source": [
    "# Load test set (can be replaced to other set/file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b031370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "695 sentences in total\n",
      "153 are opinion\n",
      "0.22014388489208633\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "sens = []\n",
    "opinion_answer = []\n",
    "driver_answer = []\n",
    "barrier_answer = []\n",
    "\n",
    "with open('UNSW_answer_without_conclusion/sen_token.json') as file_obj:\n",
    "    sen = json.load(file_obj)\n",
    "    sens.extend(sen)\n",
    "with open('UNSW_answer_without_conclusion/driver_token.json') as file_obj:\n",
    "    driver = json.load(file_obj)\n",
    "    driver_answer.extend(driver)\n",
    "    opinion_answer.extend(driver)\n",
    "with open('UNSW_answer_without_conclusion/barrier_token.json') as file_obj:\n",
    "    barrier = json.load(file_obj)\n",
    "    barrier_answer.extend(barrier)\n",
    "    opinion_answer.extend(barrier)\n",
    "with open('UNSW_answer_2/sen_token.json') as file_obj:\n",
    "    sen = json.load(file_obj)\n",
    "    sens.extend(sen)\n",
    "with open('UNSW_answer_2/driver_token.json') as file_obj:\n",
    "    driver = json.load(file_obj)\n",
    "    driver_answer.extend(driver)\n",
    "    opinion_answer.extend(driver)\n",
    "with open('UNSW_answer_2/barrier_token.json') as file_obj:\n",
    "    barrier = json.load(file_obj)\n",
    "    barrier_answer.extend(barrier)\n",
    "    opinion_answer.extend(barrier)\n",
    "    \n",
    "print(str(len(sens)) + ' sentences in total')\n",
    "print(str(len(opinion_answer)) + ' are opinion')\n",
    "print(len(opinion_answer)/len(sens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f74653dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166\n"
     ]
    }
   ],
   "source": [
    "find_opinion = find_opinion_by_lexicon(sens, 'final_200.json', 0.24)\n",
    "print(len(find_opinion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "894f4e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def calculate_metric(find, answer, sents):\n",
    "    y_actual = []\n",
    "    y_predicted = []\n",
    "    for s in sents:\n",
    "        if s in answer:\n",
    "            y_actual.append(1)\n",
    "        else:\n",
    "            y_actual.append(0)\n",
    "        if s in find:\n",
    "            y_predicted.append(1)\n",
    "        else:\n",
    "            y_predicted.append(0)\n",
    "\n",
    "    Accuracy = accuracy_score(y_actual, y_predicted)\n",
    "    Precision = precision_score(y_actual, y_predicted)\n",
    "    Recall = recall_score(y_actual, y_predicted)\n",
    "    F1_Score = f1_score(y_actual, y_predicted)\n",
    "\n",
    "    return Precision, Recall, F1_Score, Accuracy\n",
    "\n",
    "def print_detail(find, answer, sents, name):\n",
    "    print(len(sents), 'samples in total')\n",
    "    print(len(answer), name+'s in sample')\n",
    "    print(len(find), name+'s are found')\n",
    "    count = 0\n",
    "    for f in find:\n",
    "        if f in answer:\n",
    "            count += 1\n",
    "    print(count, name+'s found are right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5363b8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision, Recall, F1_Score, Accuracy\n",
      "(0.35542168674698793, 0.38562091503267976, 0.36990595611285265, 0.7107913669064748)\n",
      "695 samples in total\n",
      "153 opinions in sample\n",
      "166 opinions are found\n",
      "59 opinions found are right\n"
     ]
    }
   ],
   "source": [
    "print('Precision, Recall, F1_Score, Accuracy')\n",
    "print(calculate_metric(find_opinion, opinion_answer, sens))\n",
    "print_detail(find_opinion, opinion_answer, sens, 'opinion')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d14082d",
   "metadata": {},
   "source": [
    "## driver & barrier classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ceffb999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from utils import construct_dict\n",
    "import os\n",
    "\n",
    "if 'opinion_dic.json' in os.listdir():\n",
    "    with open('opinion_dic.json') as file_obj:\n",
    "        opinion_dic = json.load(file_obj)\n",
    "else:\n",
    "    opinion_dic = construct_dict(opinions)\n",
    "    \n",
    "driver_dic = construct_dict(drivers)\n",
    "barrier_dic = construct_dict(barriers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "873f7fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3205 words in opinion_dic\n",
      "2704 words in driver_dic\n",
      "['management', 'environmental', 'industry', 'responsibility', 'also', 'reduce', 'resource', 'need', 'order', 'solar', 'impact', 'energy', 'benefit', 'government', 'producer', 'would', 'reduction', 'necessary', 'treatment', 'ensure', 'future', 'market', 'sustainable', 'approach', 'encourage', 'important', 'effective', 'opportunity', 'circular', 'system', 'support', 'policy', 'provide', 'improve', 'essential', 'waste', 'efficient', 'model', 'help', 'raw', 'supply', 'strategy', 'needed', 'manufacturer', 'new', 'economy', 'avoid', 'developing', 'sector', 'Therefore', 'required', 'sustainability', 'must', 'collection', 'business', 'stakeholder', 'manufacturing', 'appropriate', 'environment', 'take-back', 'consider', 'chain', 'job', 'solution', 'based', 'demand', 'al.', 'potential', 'responsible', 'best', 'European', 'mean', 'aim', 'price', 'party', 'positive', 'Si', 'growth', 'reduced', 'Directive', 'plan', 'improved', 'network', 'decrease', 'human', 'driver', 'environmentally', 'research', 'become', 'development', 'minimize', 'planning', 'extended', 'EPBT', 'health', 'better', 'saving', 'increase', 'effect', 'Thus']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "driver_words = []\n",
    "driver_freq = []\n",
    "opinion_freq = []\n",
    "for dd in driver_dic.keys():\n",
    "    if dd in opinion_dic.keys():\n",
    "        driver_words.append(dd)\n",
    "        driver_freq.append(driver_dic[dd])\n",
    "        opinion_freq.append(opinion_dic[dd])\n",
    "print(len(opinion_dic.keys()), 'words in opinion_dic')\n",
    "print(len(driver_dic.keys()), 'words in driver_dic')\n",
    "\n",
    "# Sort(Normalization(Opinion_freq) - Normalization(lexicon_freq)) to construct dictionary\n",
    "\n",
    "driver_freq = np.array(driver_freq).reshape(1, -1)\n",
    "normalizer_driver = preprocessing.Normalizer().fit_transform(driver_freq)\n",
    "normalizer_driver = list(normalizer_driver[0])\n",
    "\n",
    "opinion_freq = np.array(opinion_freq).reshape(1, -1)\n",
    "normalizer_opinion = preprocessing.Normalizer().fit_transform(opinion_freq)\n",
    "normalizer_opinion = list(normalizer_opinion[0])\n",
    "\n",
    "more_value = {}\n",
    "for i, w in enumerate(driver_words):\n",
    "    more_value[w] = normalizer_driver[i]-normalizer_opinion[i]\n",
    "pri_words = sorted(more_value.items(), key=lambda item:item[1], reverse=True)\n",
    "pri_list_whole = []\n",
    "for p_set in pri_words:\n",
    "    if opinion_dic[p_set[0]]>1:\n",
    "        # print(i, p_set, opinion_dic[p_set[0]])\n",
    "        pri_list_whole.append(p_set[0])\n",
    "        \n",
    "# The amount can be adjusted, here choose 100  \n",
    "driver_pri = pri_list_whole[:100]\n",
    "print(driver_pri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fc23edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3205 words in opinion_dic\n",
      "1642 words in barrier_dic\n",
      "['lack', 'current', 'low', 'still', 'challenge', 'incentive', 'due', 'metal', 'However', 'process', 'currently', 'high', 'present', 'many', 'yet', 'amount', 'barrier', 'available', 'limited', 'activity', 'value', 'higher', 'difficult', 'recycle', 'uncertainty', 'volume', 'long', 'issue', 'face', 'little', 'product', 'absence', 'consumer', 'major', 'given', 'landfill', 'type', 'handle', 'year', 'recyclers', 'facility', 'BESS', 'concentration', 'challenging', 'small', 'rate', 'According', 'site', 'transport', 'thermal', 'problem', '“', 'fact', 'may', 'however', 'recover', 'today', 'Regulations', 'generated', 'different', 'cost', 'poor', 'usually', 'country', 'economic', 'concern', 'i.e', 'plant', 'silicon-based', 'extremely', 'uncertain', 'confidence', 'focused', 'interesting', 'CUR', 'FIR', 'IBPR', 'LIR', 'LPR', 're-cycling', 'stream', 'even', 'considered', 'addressed', 'involved', 'lifetime', 'enough', 'reason', 'quantity', 'always', 'taken', 'remain', 'technological', 'complexity', 'exist', 'layer', 'mainly', 'separation', 'solvent', 'show']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "barrier_words = []\n",
    "barrier_freq = []\n",
    "opinion_freq = []\n",
    "for dd in barrier_dic.keys():\n",
    "    if dd in opinion_dic.keys():\n",
    "        barrier_words.append(dd)\n",
    "        barrier_freq.append(barrier_dic[dd])\n",
    "        opinion_freq.append(opinion_dic[dd])\n",
    "print(len(opinion_dic.keys()), 'words in opinion_dic')\n",
    "print(len(barrier_dic.keys()), 'words in barrier_dic')\n",
    "\n",
    "# Sort(Normalization(Opinion_freq) - Normalization(lexicon_freq)) to construct dictionary\n",
    "\n",
    "barrier_freq = np.array(barrier_freq).reshape(1, -1)\n",
    "normalizer_barrier = preprocessing.Normalizer().fit_transform(barrier_freq)\n",
    "normalizer_barrier = list(normalizer_barrier[0])\n",
    "\n",
    "opinion_freq = np.array(opinion_freq).reshape(1, -1)\n",
    "normalizer_opinion = preprocessing.Normalizer().fit_transform(opinion_freq)\n",
    "normalizer_opinion = list(normalizer_opinion[0])\n",
    "\n",
    "more_value = {}\n",
    "for i, w in enumerate(barrier_words):\n",
    "    more_value[w] = normalizer_barrier[i]-normalizer_opinion[i]\n",
    "pri_words = sorted(more_value.items(), key=lambda item:item[1], reverse=True)\n",
    "pri_list_whole = []\n",
    "for p_set in pri_words:\n",
    "    if opinion_dic[p_set[0]]>1 and p_set[0] != '[' and p_set[0] != ']' and p_set[0] != '’':\n",
    "        # print(i, p_set, opinion_dic[p_set[0]])\n",
    "        pri_list_whole.append(p_set[0])\n",
    "        \n",
    "# The amount can be adjusted, here choose 100  \n",
    "barrier_pri = pri_list_whole[:100]\n",
    "print(barrier_pri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3abc95e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_(driver_p, barrier_p, opinions):\n",
    "    find_driver = []\n",
    "    for o in opinions:\n",
    "        d_score = 0\n",
    "        b_score = 0\n",
    "        for word in o:\n",
    "            if word in driver_p:\n",
    "                d_score += 1\n",
    "            if word in barrier_p:\n",
    "                b_score += 1\n",
    "        if d_score >= b_score:\n",
    "            find_driver.append(o)\n",
    "    return find_driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ee93742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "695 sentences in total\n",
      "153 are opinion\n",
      "117 are driver\n",
      "0.22014388489208633\n"
     ]
    }
   ],
   "source": [
    "sens = []\n",
    "opinion_answer = []\n",
    "driver_answer = []\n",
    "barrier_answer = []\n",
    "\n",
    "with open('UNSW_answer_without_conclusion/sen_token.json') as file_obj:\n",
    "    sen = json.load(file_obj)\n",
    "    sens.extend(sen)\n",
    "with open('UNSW_answer_without_conclusion/driver_token.json') as file_obj:\n",
    "    driver = json.load(file_obj)\n",
    "    driver_answer.extend(driver)\n",
    "    opinion_answer.extend(driver)\n",
    "with open('UNSW_answer_without_conclusion/barrier_token.json') as file_obj:\n",
    "    barrier = json.load(file_obj)\n",
    "    barrier_answer.extend(barrier)\n",
    "    opinion_answer.extend(barrier)\n",
    "with open('UNSW_answer_2/sen_token.json') as file_obj:\n",
    "    sen = json.load(file_obj)\n",
    "    sens.extend(sen)\n",
    "with open('UNSW_answer_2/driver_token.json') as file_obj:\n",
    "    driver = json.load(file_obj)\n",
    "    driver_answer.extend(driver)\n",
    "    opinion_answer.extend(driver)\n",
    "with open('UNSW_answer_2/barrier_token.json') as file_obj:\n",
    "    barrier = json.load(file_obj)\n",
    "    barrier_answer.extend(barrier)\n",
    "    opinion_answer.extend(barrier)\n",
    "    \n",
    "print(str(len(sens)) + ' sentences in total')\n",
    "print(str(len(opinion_answer)) + ' are opinion')\n",
    "print(str(len(driver_answer)) + ' are driver')\n",
    "print(len(opinion_answer)/len(sens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "386d6685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n"
     ]
    }
   ],
   "source": [
    "find_driver = db_(driver_pri, barrier_pri, opinion_answer)\n",
    "find_barrier = []\n",
    "print(len(find_driver))\n",
    "for o in opinion_answer:\n",
    "    if o not in find_driver:\n",
    "        find_barrier.append(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3f39b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision, Recall, F1_Score, Accuracy\n",
      "(0.8977272727272727, 0.6752136752136753, 0.7707317073170732, 0.6928104575163399)\n",
      "153 samples in total\n",
      "117 drivers in sample\n",
      "88 drivers are found\n",
      "79 drivers found are right\n"
     ]
    }
   ],
   "source": [
    "print('Precision, Recall, F1_Score, Accuracy')\n",
    "print(calculate_metric(find_driver, driver_answer, opinion_answer))\n",
    "print_detail(find_driver, driver_answer, opinion_answer, 'driver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e92a20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision, Recall, F1_Score, Accuracy\n",
      "(0.4153846153846154, 0.75, 0.5346534653465346, 0.6928104575163399)\n",
      "153 samples in total\n",
      "36 drivers in sample\n",
      "65 drivers are found\n",
      "27 drivers found are right\n"
     ]
    }
   ],
   "source": [
    "print('Precision, Recall, F1_Score, Accuracy')\n",
    "print(calculate_metric(find_barrier, barrier_answer, opinion_answer))\n",
    "print_detail(find_barrier, barrier_answer, opinion_answer, 'driver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca57364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
